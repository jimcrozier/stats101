\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Statistics 101},
            pdfauthor={Matt Mastin and Jim Crozier},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Statistics 101}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Matt Mastin and Jim Crozier}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{2020-03-02}

\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{intro}{%
\chapter{Introduction}\label{intro}}

This textbok accompanies an introductory statistical analysis course run by the Data Science team. It will take place over the 5 Tuesdays in March 2020 (though March 24th will be a review / catch-up day\ldots{} go vote!). The purpose of this course is to help develop a solid foundation of basic statistical intuition. We aren't going to dive too deeply into the math, but we aren't going to shy away understanding the formulas. You will get the most out of the lectures if you are comfortable with basic algebra. The course is open to anyone with manager approval. For this first iteration we will limit enrollment to 15 and the course will be remote friendly. The course is two hours per week, and the format will be a one hour lecture followed by a one hour interactive computer lab.

The confluence page for the course is located at \url{go.rsglab.com/stats101}.

\hypertarget{summarystats}{%
\chapter{Summary Statistics}\label{summarystats}}

Our first topic will be summary statistics and the basic question we want to ask is: given a list of numbers how can we describe its basic shape?

We will typically denote our list of numbers \(x\) and refer to it as a \textbf{sample} or \textbf{dataset}. We can generate a particular example in R by doing the following.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{0}\OperatorTok{:}\DecValTok{50}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DataTypeTok{replace=}\OtherTok{TRUE}\NormalTok{)}
\KeywordTok{print}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 44 29  2 35 12 33 45  4 11 37
\end{verbatim}

This will sample \(10\) numbers ``randomly'' from the the integers between \(0\) and \(50\). Random can be a loaded term and we will talk more about what it can mean later, but here it means that each number is equally likely to be selected at each draw. The \texttt{replace=TRUE} bit means that we ``put'' a number back into the pot after we draw it so that we have the possibility of drawing it again. We will talk more about the various ways samples can be taken later on in the course.

\hypertarget{frequency-histograms}{%
\section{Frequency Histograms}\label{frequency-histograms}}

We can visualize the list of numbers using a \textbf{histogram} as shown here.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(x, }\DataTypeTok{col=}\StringTok{"grey"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-2-1.pdf}

This is essentially a bar graph where the x-axis shows ranges and the y-axis gives the count of numbers in each range that appear in the sample. Or, said another way, the \emph{height} of each bar is equal to the count of numbers in each range. This is called a \textbf{frequency histogram} because it displays the \emph{number} of times each number appears on the list.

\hypertarget{how-large-are-the-numbers-on-the-list}{%
\section{How Large Are the Numbers on the List?}\label{how-large-are-the-numbers-on-the-list}}

So, what do we really mean by the size of the numbers on the list? The right definition can often depend on what you are interested in. Maybe you only need to know the range of the numbers.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{max}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 45
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{min}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2
\end{verbatim}

These measure the extremes of the dataset, but how are the numbers distributed between them? Are they piled up on one side of the other? Do they mostly fall in the middle or are they evenly spread out? We can see the distrubution on a histogram, but how do we \emph{quantify} these properties? We will do this with two numbers. One will measure where the ``center'' of the sample falls and the other measures how spread out the sample is. These are sometimes called \emph{measures of central tendency}. Let's first discuss how we can describe the center of the dataset.

\hypertarget{the-median}{%
\subsection{The Median}\label{the-median}}

The \textbf{median} of a list of numbers is the value at the midpoint if we were to arrange the numbers in order. This can also be thought of as a value separating the higher half from the lower half.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sort}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  2  4 11 12 29 33 35 37 44 45
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{median}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 31
\end{verbatim}

We can visualize the median on a frequency histogram.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mx <-}\StringTok{ }\KeywordTok{median}\NormalTok{(x)}
\KeywordTok{hist}\NormalTok{(x, }\DataTypeTok{col=}\StringTok{'grey'}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v =}\NormalTok{ mx, }\DataTypeTok{col =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{lwd =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-5-1.pdf}

A nice property of the median is that it is not very sensitive to outliers, that is, numbers that are far from the center of the dataset.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{median}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 31
\end{verbatim}

Now watch what happens we add some very large numbers to the list.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{median}\NormalTok{(}\KeywordTok{append}\NormalTok{(x, }\DecValTok{939239}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 33
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{median}\NormalTok{(}\KeywordTok{append}\NormalTok{(x, }\DecValTok{345254}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 33
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{median}\NormalTok{(}\KeywordTok{append}\NormalTok{(x, }\DecValTok{76547567}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 33
\end{verbatim}

Notice that the median isn't very sensitive to the value of an added outlier. So, when data is known to have some kind of skew it is common to use medians to describe the center. A typical example of this is income due to its concentration in the hands of relatively few.

The median also has the property that if we were to randomly select a number from our list it would be just as likely be greater than the median as less. There is an interesting relationship between statistics and probability that we will discuss more in chapter \ref{probability}.

\hypertarget{the-average}{%
\subsection{The Average}\label{the-average}}

Another way to define the center of a dataset is the average and in order to define it we will introduce a bit more notation. Recall that we are calling our list of numbers \(x\). We could reference the individual numbers in the list by \(x_i\) where \(i\) can range between \(1\) and \(n\) where \(n\) is the size of the list. The \textbf{average} or \textbf{mean} of the list \(x\) can then be defined by the following expression. Note that we are following a fairly standard convention of denoting the average by the Greek letter ``mu''. Sometimes the mean is also denoted \(\bar{x}\).

\[\mu(x) = \bar{x} = \frac{1}{n} \sum_{i=1}^n x_i\]
Here we are using \href{https://en.wikipedia.org/wiki/Summation}{summation notation} which is convenient way of writing down a sum of an arbitrary number of terms. Let's look at an example to make sure we see how this notation works.

\emph{\textbf{Example}}: Let \(x = \{1, 6, 4, 7, 8\}\), then the average of \(x\) is given by

\[\mu(x) = \frac{1}{5} \sum_{i=1}^5 x_i = \frac{1}{5} (1 + 6 + 4 + 7 + 8) = \frac{1}{5} (26) = 5.2\]

Of course, we could also let R do this for us.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{8}\NormalTok{)}
\KeywordTok{mean}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5.2
\end{verbatim}

Now let's talk about some interpretations of the average. One is that it is the number that ``balances'' the histogram in the sense depicted in this image.

\begin{figure}
\centering
\includegraphics{average_balance.png}
\caption{Averages balance histograms}
\end{figure}

As we slide the rightmost part of the histogram out the average must move with it in order to prevent the histogram from tipping. Maybe this is why the average is sometimes called the \textbf{first moment} (see the definition of a \href{https://simple.wikipedia.org/wiki/Moment_(physics)}{moment} from physics).

We can perform a similar experiment as we did with medians to look at how outliers affect the mean.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5.2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(}\KeywordTok{append}\NormalTok{(x, }\DecValTok{939239}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 156544.2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(}\KeywordTok{append}\NormalTok{(x, }\DecValTok{345254}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 57546.67
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(}\KeywordTok{append}\NormalTok{(x, }\DecValTok{76547567}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 12757932
\end{verbatim}

Unlike the median, the mean is very sensitive to outliers, but this isn't always a bad thing as we will see in a moment.

Another interpretation of the mean is that is the value for a ``typical'' number of the list. Or, more precisely, if we randomly select a number from the list the mean would be the best guess for what the value would be in the sense that it would, in the long run, be closer to the actually selected numbers than any other guess. We will talk more about this as well in chapter \ref{probability}. Notice that here we care about the outlier because we would, even if it was only occassionally, draw it from the list if we were to take enough draws.

To finish off this section we note that the relationship between the mean and median can give information about the shape of the histogram. Here is a nice graphic from the \href{https://en.wikipedia.org/wiki/Skewness}{wikipedia page} on \textbf{skewness}.
The third measure shown here is the \textbf{mode}. This is the value that occurs most often in the list, so it corresponds to the maximum value of the histogram.

\begin{figure}
\centering
\includegraphics{hist_skew.png}
\caption{Skewed histograms}
\end{figure}

\hypertarget{root-mean-square}{%
\subsection{Root Mean Square}\label{root-mean-square}}

There is another measure of size that is important, but sometimes not given the attention it deserves. The expression below defines the \textbf{root mean square} of a list.

\[RMS(x) = \sqrt{\frac{1}{n}\sum_{i=1}^n x_i^2}\]

Why do we need another way of measure the size of the list? Consider the following example.

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-10-1.pdf}

Notice that this histogram is fairly symmetric about \(0\). This means that we should expect the mean to be close to \(0\) (for example, by thinking about the balancing analogy above).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.0194491
\end{verbatim}

Notice what happens when we add balanced outliers.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(}\KeywordTok{append}\NormalTok{(x, }\KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{-10}\NormalTok{)), }\DataTypeTok{col=}\StringTok{'grey'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-12-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.0194491
\end{verbatim}

The mean is unchanged! This isn't good if we are interested in measuring the size of the \emph{magnitude} of the numbers. This is where the root mean square comes in. It is sensitive to adding balanced outliers.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{mean}\NormalTok{(x}\OperatorTok{^}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.966248
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{mean}\NormalTok{(}\KeywordTok{append}\NormalTok{(x, }\KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{-10}\NormalTok{))}\OperatorTok{^}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.06366
\end{verbatim}

It is reasonable to ask why we use this measure instead of something ``simpler'' like, for instance, taking the average of absolute values. In truth, that would also be a reasonable way to measure the size of the magnitudes of the numbers (average absolute deviation is a fairly common way to measure error). However, the root mean square is related to the notion of the norm of a vector in the \textbf{Euclidean space} \(\mathbb{R}^n\) which, as we will see in chapter \ref{corr}, can be exploited. If this is unfamiliar, then the takeaway is that this formula fits into a larger framework and this is convenient.

\hypertarget{how-spread-out-are-the-numbers}{%
\section{How Spread Out Are the Numbers?}\label{how-spread-out-are-the-numbers}}

Now that we know a few ways to describe the center of a list of numbers we can talk about measuring how the numbers are arranged around the center. Generally speaking we will use the mean as our definition of center.

\hypertarget{standard-deviation}{%
\subsection{Standard Deviation}\label{standard-deviation}}

The \textbf{deviation} of a number on our list is defined as the difference between the number and the mean of the sample. The \textbf{standard deviation} is defined to be the root mean square of the deviations.

We can write the standard deviation formally as the following expression.

\[\sigma(x) = \sqrt{\frac{1}{n}\sum_{i-1}^n (x_i - \bar{x})^2}\]
Let's revisit the list we used in the mean example above, but this time compute the standard deviation. Recall that our list is \(x = \{1, 6, 4, 7, 8\}\) and the average of the list was \(5.2\).

\[\sigma(x) = \sqrt{\frac{1}{5} \sum_{i=1}^5 (x_i - 5.2)^2} = \]
\[ = \sqrt{ \frac{1}{5} ((1 - 5.2)^2 + (6 - 5.2)^2 + (4 - 5.2)^2 + (7 - 5.2)^2 + (8 - 5.2)^2)} \approx 2.48 \]

We can also do this using R.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{8}\NormalTok{)}
\NormalTok{deviations <-}\StringTok{ }\NormalTok{x }\OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(x)}
\KeywordTok{print}\NormalTok{(deviations)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -4.2  0.8 -1.2  1.8  2.8
\end{verbatim}

Now that we have the deviations we can take the root mean square to obtain the standard deviation.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{mean}\NormalTok{(deviations}\OperatorTok{^}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2.481935
\end{verbatim}

R does have a built in \texttt{sd} function. Let's give that a shot.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sd}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2.774887
\end{verbatim}

This value is different because this function in R is actually what is typically called the \textbf{sample standard deviation}. What we defined above is technically called the \textbf{population standard deviation}. The sample standard deviation is given by the following.

\[S(x) = \sqrt{\frac{1}{n-1}\sum_{i-1}^n (x_i - \bar{x})^2}\]

The difference between the population and sample standard deviations is outside of our scope for now, but the fact that there are two notions of standard deviations is something to be aware of. We will go into more detail when we discuss probability in chapter \ref{probability}.

\hypertarget{variance}{%
\subsection{Variance}\label{variance}}

The variance of a list of numbers is defined to be the square of the standard deviation and, as such, is typically denoted \(\sigma^2\).

\[\sigma^2(x) = \frac{1}{n}\sum_{i-1}^n (x_i - \bar{x})^2\]

There are instances when working with a quantity that contains a square root is inconvenient and often using the variance can lead to simpler arithmetic, but it carries essentially the same information as the standard deviation.

\hypertarget{density-histograms}{%
\section{Density Histograms}\label{density-histograms}}

As we previously mentioned, frequency histograms are essentialy just bar charts. This means that the scale of the y-axis depends on the numbers appear in our list. This can make it difficult to reason about the data or compare datasets to each other. For example, given the following histogram how could we quickly estimate the percentage of numbers that fall between \(1\) and \(2\)?

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-17-1.pdf}

We would need to sum up the heights of each bar between \(1\) and \(2\) and then divide by the size of the list. This isn't particularly difficult, but we can make this more easily read off the histogram. A \textbf{density histogram} is one where the the \emph{area} of each bar corresponds to the \emph{percentage} of numbers that fall within each range. If we restrict ourselves to histograms with bins of all the same width, then we can obtain the density histogram by dividing the frequency of each number by the sample size.

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-18-1.pdf}

To estimate the percentage of the numbers that fall between \(1\) and \(2\) we can estimate the total area of the bars between \(1\) and \(2\). Each bar has width \(0.5\), so the area of the bar between \(1\) and \(1.5\) is about \(0.5 * 0.2 = 0.1\) and the bar between \(1.5\) and \(2\) is has area about \(0.5 * 0.1 = 0.05\). So, the percentage of the numbers that fall between \(1\) and \(2\) is about \(15\%\).

Density histograms will play an important role in chapter \ref{probability} when we talk about the relationship between probability and statistics.

\hypertarget{standardization}{%
\section{Standardization}\label{standardization}}

We can think of converting from a frequency histogram to a density histogram as a way of ``standardizing'' the data. In other words, we rescale so that the y-axes of all of our histograms have the same scale. It is often also convenient to rescale the data itself. We will give an example of why we might want to do this shortly, but first we will describe a typical way in which this is done.

Consider the following density histogram.

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-19-1.pdf}

To standardize this histogram we will make two modifications. The first will be to compute the mean of the data and then subtract that from each number in the sample. Let's do this using R and then look at the resulting histogram.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\NormalTok{x }\OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(x)}
\KeywordTok{hist}\NormalTok{(x, }\DataTypeTok{col=}\StringTok{'grey'}\NormalTok{, }\DataTypeTok{freq =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{main=}\StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-20-1.pdf}

Notice that the histogram is now centered at \(0\) instead of whatever the mean of the original dataset was. It can be nice to center histograms at \(0\) if we are comparing the shape of data across datasets that may have different means (we will look at an example of this in the next section). It is interesting to note that the mean of the dataset after subtracting the original mean is \(0\).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.853411e-15
\end{verbatim}

Note that this number is in \href{https://en.wikipedia.org/wiki/Scientific_notation}{scientific notation} and is very close to zero. It is not exactly zero due to rounding error in the numerical computation.

The second modification we will make is to divide each number in the sample by the standard deviation of the sample. Let's do this and then look at the effect on the histogram.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\NormalTok{x }\OperatorTok{/}\StringTok{ }\KeywordTok{sd}\NormalTok{(x)}
\KeywordTok{hist}\NormalTok{(x, }\DataTypeTok{col=}\StringTok{'grey'}\NormalTok{, }\DataTypeTok{freq =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{main=}\StringTok{"Standardized Histogram"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-22-1.pdf}

Notice that this standardized histogram has the same shape as the previous one, but it has been rescaled so that the majority of the values fall between \(-2\) and \(2\).

We can interpret the rescaled data to be telling us how many standard deviations from the mean each datapoint in the original dataset falls. Sometimes these are called \textbf{standard units} and this is also related to the \textbf{z-statistic} that is used in hypothesis tests. But, this is a topic for chapter \ref{hypothesis}.

In the next section we will look at an example of how standardizing data can be a powerful tool for data analysis.

\hypertarget{example}{%
\section{Example}\label{example}}

Consider the following situation motivated by a real situation within Mailchimp data. We have many users and each of their email campaigns have an associated open rate. Below we see density histograms for three different users showing the distributions of their open rates across all campaigns they have sent.

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-24-1.pdf}

We can see from these histograms that the mean open rates for these users are about \(20\), \(30\), and \(60\) respectively. We can also see that the variances around the means are different for each user. This makes sense since each user's audience will engage at different rates across campaigns.

Now suppose we want to analyze the impact of some Mailchimp feature on open rate. We could split each histogram by whether or not the campaigns used the feature.

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-26-1.pdf}

Now we have two histograms for each of these users. One showing open rate using the feature and the other showing open rate \emph{not} using the feature. It appears that there is a difference between open rates, but the actual amount of change seems to vary between the users. So, how can we quantify how much the feature impacts open rate?

An issue here is that it is difficult to separate the impact of the feature from the natural difference between audience engagement across these users. Let's try standardizing the histograms and see what happens.

We don't want to standardize using \texttt{sd(x\_1\_a)} and \texttt{mean(x\_1\_a)} because we would

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-27-1.pdf}

After standardizing we can see that the histograms corresponding to using the feature have collapsed to almost the same histogram and similarly for the histograms corresponding to not using the feature. The peaks of the histograms corresponding to using the feature are at about \(1\) standard unit and the peaks of the histograms corresponding to not using the feature are at about \(-1\) standard unit. We can interpret this as telling us that that if a user uses the feature they should expect for the campaign to perform at about \(1\) standard deviation above their mean open rate. Similarly, if a user does not use the feature then they should expect the campaign to perform at about \(1\) standard deviation below their mean open rate.

Note that the data used in this example was fake and generated so that these standardized histograms lined up well and the separation between using and not using the feature was large. Real data is generally much noisier than this and the effect of feature usage is much harder to tease out.

There is one more important thing to note about this example. In order to get an idea of the feature's impact we need to standardize in a slightly different way than was described above. We have split each users data into two groups, but we will standardize each group with respect to the overall mean and standard deviation of the user's campaign open rate data as show below. Note that \texttt{x\_1\_a} is the data from campaigns sent by this user that did not use the feature in question.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{standardized_data <-}\StringTok{ }\NormalTok{(x_}\DecValTok{1}\NormalTok{_a }\OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(x_}\DecValTok{1}\NormalTok{)) }\OperatorTok{/}\StringTok{ }\KeywordTok{sd}\NormalTok{(x_}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

If we had instead standardized using the mean and standard deviation of \texttt{x\_1\_a} then we would have ended up with data that was mean \(0\) and variance \(1\) as we noted previously. So, all of the histograms would have piled on top of each other and we would have hidden the actual impact of the feature from ourselves as shown below.

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-29-1.pdf}

\hypertarget{lab-questions}{%
\section{Lab Questions}\label{lab-questions}}

\hypertarget{introduction-to-r}{%
\subsection{Introduction to R}\label{introduction-to-r}}

R is a high level statistical computing language. We will be using RStudio as an development environment for this course. I am borrowing a lot from this great book on \href{https://r4ds.had.co.nz/index.html}{R for Data Science} , this \href{https://github.com/CerebralMastication/r_for_the_student}{overview}, as well as some stuff from this great \href{https://bookdown.org/content/3686/the-r-programming-language.html}{bayes analysis book}.

Before we get started, let's make some appearance changes that will make the screen shots easier to see.

\begin{figure}
\centering
\includegraphics{./pics/lab1_ss1.png}
\caption{Choose Tools}
\end{figure}

\includegraphics{./pics/lab1_ss2.png}
\includegraphics{./pics/lab1_ss3.png}

Whew, that's better. Now, let's get to know the development environment. First create a file, and then we step through the sections

\includegraphics{./pics/lab1_ss4.png}
We will walk through these one by one in the lab

\begin{figure}
\centering
\includegraphics{./pics/lab1_ss5.png}
\caption{Create file}
\end{figure}

\hypertarget{getting-help}{%
\subsection{Getting help}\label{getting-help}}

\begin{verbatim}
## starting httpd help server ... done
\end{verbatim}

\begin{verbatim}
## If the browser launched by '/usr/bin/open' is already running, it
##     is *not* restarted, and you must switch to its window.
## Otherwise, be patient ...
\end{verbatim}

shows many of the help documentation that is available. You can also get help with most any function with by prepending \texttt{?} to the funcion call. For instance, to get help with the sample() function in base R,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{?}\KeywordTok{sample}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

And the documentation will show in the Help tab.

\hypertarget{running-code}{%
\subsection{Running code}\label{running-code}}

You can run code three ways:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Type code directly into the RELP (Read--eval--print loop)
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(}\StringTok{"hello world"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "hello world"
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  Type code into the text editor, and hit Run (top left) (you can also run your current line by hitting cmd + enter, see \href{https://support.rstudio.com/hc/en-us/articles/200711853-Keyboard-Shortcuts}{here} for more keyboard shortcuts).
\item
  Saving your code and calling the saved code from the command line.
\end{enumerate}

\hypertarget{tutorial-quick-overview}{%
\subsection{Tutorial : Quick overview}\label{tutorial-quick-overview}}

On the command line run the following command:

\texttt{learnr::run\_tutorial("R\_Basics",\ "sur")}

and follow the prompts.

\hypertarget{functions}{%
\subsection{Functions}\label{functions}}

Functions allow you make code repeatable and callable. We will be learning more about functions in later

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{print_str =}\StringTok{ }\ControlFlowTok{function}\NormalTok{(str)\{}
\KeywordTok{print}\NormalTok{(str)}
\NormalTok{\}}

\KeywordTok{print_str}\NormalTok{(}\StringTok{"hello, world"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "hello, world"
\end{verbatim}

The inputs to the function can be given a default.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{print_str =}\StringTok{ }\ControlFlowTok{function}\NormalTok{(}\DataTypeTok{str=}\StringTok{"intial string"}\NormalTok{)\{}
\KeywordTok{print}\NormalTok{(str)}
\NormalTok{\}}

\KeywordTok{print_str}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "intial string"
\end{verbatim}

if you pass an argument, it will override the default

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{print_str =}\StringTok{ }\ControlFlowTok{function}\NormalTok{(}\DataTypeTok{str=}\StringTok{"intial string"}\NormalTok{)\{}
\KeywordTok{print}\NormalTok{(str)}
\NormalTok{\}}

\KeywordTok{print_str}\NormalTok{(}\StringTok{"something clever"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "something clever"
\end{verbatim}

Typically functions are used to perform an actions, or series of actions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{add_two_numbers =}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x,y)\{}
  \KeywordTok{return}\NormalTok{(x }\OperatorTok{+}\StringTok{ }\NormalTok{y)}
\NormalTok{\}}

\KeywordTok{add_two_numbers}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3
\end{verbatim}

\hypertarget{variable-assignment-and-types}{%
\subsection{Variable assignment and types}\label{variable-assignment-and-types}}

Variables can assigned with = operator and -\textgreater{} and \textless- operators (always remember that the arrow points to the variable, also the -\textgreater{} is not often used).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a1 =}\StringTok{ }\DecValTok{10}
\NormalTok{a2 <-}\StringTok{ }\DecValTok{10}

\NormalTok{a1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 10
\end{verbatim}

This list is not exhaustive, but in general you will be working with five type of variables:
1. strings
2. numeric
3. dates
4. data.frame: rectangular data, think a spreadsheet
5. factors: categorical variables, distinct buckets like ``big'', ``small''
6. lists: list are hierarchical holders of different data types

\hypertarget{packages}{%
\subsection{Packages}\label{packages}}

In R you can compile a series of functions and data into a package. Creating packages is outside the scope of this course, but if you want to get good at R, going through the process of creating packages is one the best routes. There are thousands of packages available that people have put together. For instance, \texttt{ggplot} is a common graphing package. To install a pacakge run \texttt{install.packages("ggplot")}. To load a package, run \texttt{library(ggplot)}. For instance,

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Attaching package: 'dplyr'
\end{verbatim}

\begin{verbatim}
## The following objects are masked from 'package:stats':
## 
##     filter, lag
\end{verbatim}

\begin{verbatim}
## The following objects are masked from 'package:base':
## 
##     intersect, setdiff, setequal, union
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(bigrquery)}
\KeywordTok{library}\NormalTok{(DBI)}
\KeywordTok{library}\NormalTok{(plotly)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Loading required package: ggplot2
\end{verbatim}

\begin{verbatim}
## 
## Attaching package: 'plotly'
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:ggplot2':
## 
##     last_plot
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:stats':
## 
##     filter
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:graphics':
## 
##     layout
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(gapminder)}
\end{Highlighting}
\end{Shaded}

these commands would load some commonly used packages for statisical computing, some bigquery and SQL commands, and another plotting package. Notice that these packages will only load if you have installed them, but you will only need to install them once.

\hypertarget{loading-data}{%
\subsection{Loading data}\label{loading-data}}

Let's create a csv to work on. First create an empty code file

\begin{figure}
\centering
\includegraphics{./pics/lab1_ss6.png}
\caption{Create file}
\end{figure}

Copy and paste, or type the following in the file:

\begin{verbatim}
a,b,c
1,2,3
4,5,6
7,8,9
\end{verbatim}

\begin{figure}
\centering
\includegraphics{./pics/lab1_ss7.png}
\caption{Save file}
\end{figure}

Save the file as \texttt{data.csv}, a prompt will ask you if \emph{really} want to do that (change file type), say yes. Next, let's load that file into memory. Close the csv, and create another empty R file as above. In the new file type the following

\begin{verbatim}
dat = read_csv("./data.csv")
\end{verbatim}

Hit \texttt{Run} (or cmd + enter) and you will get some parsing information print to the screen, and a new element in memory called dat (because that is what you named it). To inspect it, click on the data element in the Environment tab

\begin{figure}
\centering
\includegraphics{./pics/lab1_ss8.png}
\caption{View data}
\end{figure}

\hypertarget{tutorial-1-data-basics}{%
\subsection{Tutorial 1: Data Basics}\label{tutorial-1-data-basics}}

On the command line run the following command:

\texttt{learnr::run\_tutorial("ex-data-basics",\ "learnr")}

and follow the prompts.

\hypertarget{tidyverse-packages}{%
\subsubsection{Tidyverse Packages}\label{tidyverse-packages}}

(The following is borrowed from the R From Students mentioned above)

When a user installs the Tidyverse, 19 packages are installed . When the user loads the tidyverse using \texttt{library(tidyverse)} a core subset of 8 packages are loaded into R. To use any of the packages not loaded with the core Tidyverse, a user must explicitly load those packages (e.g.~\texttt{library(readxl)}) or call the packages using the package name prefix (e.g.~\texttt{readxl::read\_xlsx()} to run the \texttt{read\_xlsx()} function from the \texttt{readxl} package).

The packages listed below are in the ``Core Tidyverse'' and loaded with \texttt{library(tidyverse)}.

\textbf{\emph{Core Tidyverse}}

\texttt{ggplot2}: data visualization\\
\texttt{dplyr}: data manipulation\\
\texttt{tidyr}: data reshaping\\
\texttt{readr}: data import\\
\texttt{purrr}: functional programming\\
\texttt{tibble}: tidy dataframes\\
\texttt{stringr}: string manipulation\\
\texttt{forcats}: factor use

\textbf{\emph{Additional Tidyverse}}

There are 11 additional Tidyverse packages that install, but do not automatically load.

\emph{Import}

\texttt{readxl}: reading Excel files\\
\texttt{haven}: reading SPSS, Stata, and SAS data\\
\texttt{jsonlite}: manipulating JSON\\
\texttt{xml2}: reading xml\\
\texttt{httr}: accessing web APIs\\
\texttt{rvest}: web scraping\\
\texttt{feather}: data sharing with Python and beyond

\emph{Wrangle}

\texttt{lubridate}: date manipulation\\
\texttt{hms}: time-of-day manipulation

\emph{Modeling}

\texttt{modelr}: modeling pipelines\\
\texttt{broom}: takes model results and makes them tidy

More info on each can be found at \url{https://tidyverse.tidyverse.org}.

\hypertarget{dplyr-verbs}{%
\subsection{dplyr Verbs}\label{dplyr-verbs}}

Almost any time a student works with data, they will need to manipulate that data in some way. Below, we will introduce the main 6 \texttt{dplyr} verbs to help wrangle data to gain additional insight. These verbs, \texttt{select}, \texttt{filter}, \texttt{mutate}, \texttt{group\_by}, \texttt{summarize}, \texttt{arrange} are explained in detail below.

In order to motivate use of the aforementioned verbs, we will look to answer the following question:

\textbf{What is the average GDP per country since 1980?}

\hypertarget{verb-1-select}{%
\subsubsection{Verb 1: select}\label{verb-1-select}}

To begin, we only need to work with certain columns. The relevant columns to this question are \texttt{country}, \texttt{year}, \texttt{pop}, and \texttt{gdpPercap}. We can make this selection using the \texttt{select} function. The first argument in the \texttt{select} function is the data we wish to select from. The subsequent arguments are the names of the columns from the data we will select. In the code below, we save our selected columns into a new \texttt{tibble} called \texttt{gapminder\_selected}. We use a different name for the output data frame so as to not overwrite the original data frame object.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder_selected =}\StringTok{ }\KeywordTok{select}\NormalTok{(gapminder, country, year, pop, gdpPercap)}
\NormalTok{gapminder_selected }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1,704 x 4
##    country      year      pop gdpPercap
##    <fct>       <int>    <int>     <dbl>
##  1 Afghanistan  1952  8425333      779.
##  2 Afghanistan  1957  9240934      821.
##  3 Afghanistan  1962 10267083      853.
##  4 Afghanistan  1967 11537966      836.
##  5 Afghanistan  1972 13079460      740.
##  6 Afghanistan  1977 14880372      786.
##  7 Afghanistan  1982 12881816      978.
##  8 Afghanistan  1987 13867957      852.
##  9 Afghanistan  1992 16317921      649.
## 10 Afghanistan  1997 22227415      635.
## # ... with 1,694 more rows
\end{verbatim}

Once we view the data, we see that we still have 1704 rows but only 4 columns.

\hypertarget{verb-2-filter}{%
\subsubsection{Verb 2: filter}\label{verb-2-filter}}

To further answer our question, we need to filter our data down to the years of interest. We can achieve this goal using the \texttt{filter} function. Like the \texttt{select} function, the first argument in the \texttt{filter} function is the data and subsequent argument is the logical statement of which you wish to filter. In the code below, we filter our selected data and save our filtered data into a new tibble called \texttt{gapminder\_filtered}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder_filtered =}\StringTok{ }\KeywordTok{filter}\NormalTok{(gapminder_selected, year }\OperatorTok{>=}\StringTok{ }\DecValTok{1980}\NormalTok{)}
\NormalTok{gapminder_filtered}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 852 x 4
##    country      year      pop gdpPercap
##    <fct>       <int>    <int>     <dbl>
##  1 Afghanistan  1982 12881816      978.
##  2 Afghanistan  1987 13867957      852.
##  3 Afghanistan  1992 16317921      649.
##  4 Afghanistan  1997 22227415      635.
##  5 Afghanistan  2002 25268405      727.
##  6 Afghanistan  2007 31889923      975.
##  7 Albania      1982  2780097     3631.
##  8 Albania      1987  3075321     3739.
##  9 Albania      1992  3326498     2497.
## 10 Albania      1997  3428038     3193.
## # ... with 842 more rows
\end{verbatim}

Once we view the data, we see that our data now consist of 852 rows. This represents the rows of data since 1980.

\hypertarget{tutorial-2-filter}{%
\subsection{Tutorial 2: filter}\label{tutorial-2-filter}}

On the command line run the following command:

\texttt{learnr::run\_tutorial("ex-data-filter",\ "learnr")}

and follow the prompts.

\hypertarget{verb-3-mutate}{%
\subsubsection{Verb 3: mutate}\label{verb-3-mutate}}

The next step in answering our question is creating a column that contains the GDP. The \texttt{mutate} function creates new columns according to a specific function that we provide. To answer our question, we need to determine the GDP in each year. To find the GDP, we need to multiply the \texttt{gdpPercap} by the \texttt{pop}. Similar to the previous two verbs, the first argument in the \texttt{mutate} function is the data. Subsequent arguments are columns you wish to create with corresponding formulas. In the code below, we mutate our filtered data and save our mutated data into a new tibble called \texttt{gapminder\_mutated}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder_mutated =}\StringTok{ }\KeywordTok{mutate}\NormalTok{(gapminder_filtered, }\DataTypeTok{GDP =}\NormalTok{ gdpPercap }\OperatorTok{*}\StringTok{ }\NormalTok{pop)}
\NormalTok{gapminder_mutated}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 852 x 5
##    country      year      pop gdpPercap          GDP
##    <fct>       <int>    <int>     <dbl>        <dbl>
##  1 Afghanistan  1982 12881816      978. 12598563401.
##  2 Afghanistan  1987 13867957      852. 11820990309.
##  3 Afghanistan  1992 16317921      649. 10595901589.
##  4 Afghanistan  1997 22227415      635. 14121995875.
##  5 Afghanistan  2002 25268405      727. 18363410424.
##  6 Afghanistan  2007 31889923      975. 31079291949.
##  7 Albania      1982  2780097     3631. 10094200603.
##  8 Albania      1987  3075321     3739. 11498418358.
##  9 Albania      1992  3326498     2497.  8307722183.
## 10 Albania      1997  3428038     3193. 10945912519.
## # ... with 842 more rows
\end{verbatim}

Once we view the data, we see the new column, \texttt{GDP}, has been added to the end of our tibble.

\hypertarget{tutorial-3-mutate}{%
\subsection{Tutorial 3: mutate}\label{tutorial-3-mutate}}

On the command line run the following command:

\texttt{learnr::run\_tutorial("ex-data-mutate",\ "learnr")}

and follow the prompts.

\hypertarget{verb-4-group_by}{%
\subsubsection{Verb 4: group\_by}\label{verb-4-group_by}}

The next step will be to group our data by the field of interest. In this instance, since we want to know GDP by country, we need to group the data by country. A way to conceptualize this step is to think of placing each group of data into a specific room. In subsequent steps we will apply a function to each group (or room) of data. Just like the previous verbs, the first argument in the \texttt{group\_by} function is the data. The following arguments are the columns you wish to group by. In the code below, we group our mutated data and save our grouped data into a new tibble called \texttt{gapminder\_grouped}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder_grouped =}\StringTok{ }\KeywordTok{group_by}\NormalTok{(gapminder_mutated, country) }
\NormalTok{gapminder_grouped}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 852 x 5
## # Groups:   country [142]
##    country      year      pop gdpPercap          GDP
##    <fct>       <int>    <int>     <dbl>        <dbl>
##  1 Afghanistan  1982 12881816      978. 12598563401.
##  2 Afghanistan  1987 13867957      852. 11820990309.
##  3 Afghanistan  1992 16317921      649. 10595901589.
##  4 Afghanistan  1997 22227415      635. 14121995875.
##  5 Afghanistan  2002 25268405      727. 18363410424.
##  6 Afghanistan  2007 31889923      975. 31079291949.
##  7 Albania      1982  2780097     3631. 10094200603.
##  8 Albania      1987  3075321     3739. 11498418358.
##  9 Albania      1992  3326498     2497.  8307722183.
## 10 Albania      1997  3428038     3193. 10945912519.
## # ... with 842 more rows
\end{verbatim}

Students will notice that there appears to be no change to the data. This is mostly true as we have simply told R that we would like to apply future functions to each group of data instead of to the entire tibble. The only difference in output is a note explaining what the data has been grouped into and the number of groups. In this case, it explains the data is grouped by \texttt{country} and that there are 142 groups.

\hypertarget{verb-5-summarise}{%
\subsubsection{Verb 5: summarise}\label{verb-5-summarise}}

Next, in order to determine the average GDP by country, we need to apply a function to each group we have identified. Specifically, we will need to take the average GDP over each country. Since we have already grouped by country, we next need to apply the summarise function. Like the previous verbs, the first argument in the \texttt{summarise} function is the data. The following arguments are the function to apply to each group. In the code below, we summarise the grouped data and save the summarised data into a new tibble called \texttt{gapminder\_summarised}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder_summarised =}\StringTok{ }\KeywordTok{summarise}\NormalTok{(gapminder_grouped, }\DataTypeTok{AVG_GDP =} \KeywordTok{mean}\NormalTok{(GDP))}
\NormalTok{gapminder_summarised}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 142 x 2
##    country           AVG_GDP
##    <fct>               <dbl>
##  1 Afghanistan  16430025591.
##  2 Albania      13062766192.
##  3 Algeria     148613140752.
##  4 Angola       28940373965.
##  5 Argentina   353071702131.
##  6 Australia   477639321504.
##  7 Austria     225388780040.
##  8 Bahrain      12397050418.
##  9 Bangladesh  119954904364.
## 10 Belgium     271944511091.
## # ... with 132 more rows
\end{verbatim}

We now have an answer to our question. The tibble above shows the average GDP per country since 1980.

\hypertarget{tutorial-4-summarize}{%
\subsection{Tutorial 4: summarize}\label{tutorial-4-summarize}}

On the command line run the following command:

\texttt{learnr::run\_tutorial("ex-data-summarise",\ "learnr")}

and follow the prompts.

\hypertarget{verb-6-arrange}{%
\subsubsection{Verb 6: arrange}\label{verb-6-arrange}}

However, we can refine our result to provide more understanding. Currently, our data is sorted alphabetically by country. This does not provide much insight. We can use the \texttt{arrange} function to sort the data by average GDP; either ascending or descending. Like all other verbs, the first argument in the \texttt{arrange} function is the data. The following arguments are the one or more columns which you wish to sort by. In the code below, we arrange our summarised data and save our arranged data into a new tibble called \texttt{gapminder\_arranged}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder_arragned =}\StringTok{ }\KeywordTok{arrange}\NormalTok{(gapminder_summarised, AVG_GDP)}
\NormalTok{gapminder_arragned}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 142 x 2
##    country                      AVG_GDP
##    <fct>                          <dbl>
##  1 Sao Tome and Principe     213138942.
##  2 Comoros                   585013190.
##  3 Guinea-Bissau             788263198.
##  4 Gambia                    806874307.
##  5 Djibouti                  894915489.
##  6 Liberia                  1274437021.
##  7 Lesotho                  2041066151.
##  8 Equatorial Guinea        2131596160.
##  9 Eritrea                  2545841271.
## 10 Central African Republic 2670573945.
## # ... with 132 more rows
\end{verbatim}

We see, from the output above, the countries with the smallest average GDP since 1980.

It may be more interesting, however, to sort the average GDP in descending order so we can learn which countries have the highest average GDP. To do this we simply place a - sign in front of \texttt{AVG\_GDP} in the code above.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder_arragned_descending =}\StringTok{ }\KeywordTok{arrange}\NormalTok{(gapminder_summarised, }\OperatorTok{-}\NormalTok{AVG_GDP)}
\NormalTok{gapminder_arragned_descending}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 142 x 2
##    country        AVG_GDP
##    <fct>            <dbl>
##  1 United States  9.20e12
##  2 Japan          3.28e12
##  3 China          2.96e12
##  4 Germany        2.20e12
##  5 United Kingdom 1.48e12
##  6 France         1.48e12
##  7 India          1.39e12
##  8 Italy          1.33e12
##  9 Brazil         1.27e12
## 10 Mexico         9.26e11
## # ... with 132 more rows
\end{verbatim}

\hypertarget{simplifying-code-with-the-pipe-operator}{%
\subsubsection{\texorpdfstring{Simplifying Code with the Pipe Operator: \texttt{\%\textgreater{}\%}}{Simplifying Code with the Pipe Operator: \%\textgreater\%}}\label{simplifying-code-with-the-pipe-operator}}

After helping learners see how each function works we can introduce the pipe operator (\texttt{\%\textgreater{}\%}). This helpful code chains together commands and passes the results of one function directly into the next function. This results in very logical data manipulation steps that are fairly easy to learn and makes the code easy to understand:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder_arragned_descending_chained =}
\StringTok{  }\NormalTok{gapminder }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(country, year, pop, gdpPercap) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(year }\OperatorTok{>=}\StringTok{ }\DecValTok{1980}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{GDP =}\NormalTok{ gdpPercap }\OperatorTok{*}\StringTok{ }\NormalTok{pop) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(country) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{AVG_GDP =} \KeywordTok{mean}\NormalTok{(GDP)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{arrange}\NormalTok{(}\OperatorTok{-}\NormalTok{AVG_GDP)}
\NormalTok{gapminder_arragned_descending_chained}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 142 x 2
##    country        AVG_GDP
##    <fct>            <dbl>
##  1 United States  9.20e12
##  2 Japan          3.28e12
##  3 China          2.96e12
##  4 Germany        2.20e12
##  5 United Kingdom 1.48e12
##  6 France         1.48e12
##  7 India          1.39e12
##  8 Italy          1.33e12
##  9 Brazil         1.27e12
## 10 Mexico         9.26e11
## # ... with 132 more rows
\end{verbatim}

\hypertarget{lecture-questions}{%
\subsection{Lecture questions:}\label{lecture-questions}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Run the following R code many times and notice that the answer is always an integer.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{median}\NormalTok{(}\KeywordTok{sample}\NormalTok{(}\OperatorTok{-}\DecValTok{10}\OperatorTok{:}\DecValTok{10}\NormalTok{,}\DecValTok{5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -3
\end{verbatim}

Now run this snippet a few times and notice that sometimes the answer is an integer, but sometimes it's a half integer. Why is this?

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{median}\NormalTok{(}\KeywordTok{sample}\NormalTok{(}\OperatorTok{-}\DecValTok{10}\OperatorTok{:}\DecValTok{10}\NormalTok{,}\DecValTok{6}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -2
\end{verbatim}

```

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  Run an experiment to verify that fact that if we were to randomly select a number from our list it would be just as likely that the number be greater than the median as less than the median.
\item
  Show that a standardized dataset has mean \(0\) and standard deviation \(1\). You can either do this formally or by looking at some examples in R.
\end{enumerate}

\hypertarget{corr}{%
\chapter{Correlation and Regression}\label{corr}}

\hypertarget{scatter-plots}{%
\section{Scatter Plots}\label{scatter-plots}}

\hypertarget{section}{%
\section{}\label{section}}

\hypertarget{probability}{%
\chapter{Probability}\label{probability}}

Some \emph{significant} applications are demonstrated in this chapter.

\hypertarget{example-one}{%
\section{Example one}\label{example-one}}

\hypertarget{example-two}{%
\section{Example two}\label{example-two}}

\hypertarget{hypothesis}{%
\chapter{Hypothesis Tests}\label{hypothesis}}

We have finished a nice book.

\bibliography{book.bib,packages.bib}


\end{document}
